---
title: Quickstart
description: Get started with Prefect, the easiest way to orchestrate and observe your data pipelines
---

import Installation from '/snippets/installation.mdx'

Prefect is an orchestration and observability platform that empowers developers to build and scale workflows quickly.
In this quickstart, you will use Prefect to convert the following Python script to a schedulable, observable, resilient, and deployable workflow in minutes:

```python
import httpx

def get_repo_info():
    """Fetch statistics about the Prefect repository"""
    url = "https://api.github.com/repos/PrefectHQ/prefect"
    response = httpx.get(url)
    repo = response.json()
    print("PrefectHQ/prefect repository statistics ðŸ¤“:")
    print(f"Stars ðŸŒ  : {repo['stargazers_count']}")

if __name__ == "__main__":
    get_repo_info()
```

## Install Prefect

<Installation />

<Tip>
See [Install Prefect](/3.0/get-started/install/) for more details on installation.
</Tip>

## Connect to a Prefect API

Connect to a Prefect API:

<Tabs>
  <Tab title="Self-hosted">
1. Start a local API server:

   ```bash
   prefect server start
   ```

1. Open the Prefect dashboard in your browser at [http://localhost:4200](http://localhost:4200).
  </Tab>
  <Tab title="Prefect Cloud">
1. Head to [https://app.prefect.cloud/](https://app.prefect.cloud/) and sign in or create a forever-free Prefect Cloud account.
1. Log in to Prefect Cloud from your development environment:

   ```bash
   prefect cloud login
   ```

1. Choose **Log in with a web browser** and click the **Authorize** button in the browser window that opens.

Your CLI is now authenticated with your Prefect Cloud account through a locally stored API key that expires in 30 days.

If you have any issues with browser-based authentication, you can [authenticate with a manually created API key](/3.0/manage/cloud/manage-users/api-keys/) instead.
  </Tab>
</Tabs>

## Convert your script to a Prefect workflow

The easiest way to convert a Python script into a workflow is to add a `@flow` decorator to the script's entrypoint.
This will create a corresponding [flow](/3.0/develop/write-flows/). 

Adding `@task` decorators to any functions called by the flow converts them to [tasks](/3.0/develop/write-tasks/). 
Tasks receive metadata about upstream dependencies and the state of those dependencies before they run.
Prefect will then record these dependencies and states as it orchestrates these tasks.

```python my_gh_workflow.py
import httpx   # an HTTP client library and dependency of Prefect
from prefect import flow, task

@task(retries=2)
def get_repo_info(repo_owner: str, repo_name: str):
    """Get info about a repo - will retry twice after failing"""
    url = f"https://api.github.com/repos/{repo_owner}/{repo_name}"
    api_response = httpx.get(url)
    api_response.raise_for_status()
    repo_info = api_response.json()
    return repo_info

@task
def get_contributors(repo_info: dict):
    """Get contributors for a repo"""
    contributors_url = repo_info["contributors_url"]
    response = httpx.get(contributors_url)
    response.raise_for_status()
    contributors = response.json()
    return contributors

@flow(log_prints=True)
def log_repo_info(repo_owner: str = "PrefectHQ", repo_name: str = "prefect"):
    """
    Given a GitHub repository, logs the number of stargazers
    and contributors for that repo.
    """
    repo_info = get_repo_info(repo_owner, repo_name)
    print(f"Stars ðŸŒ  : {repo_info['stargazers_count']}")

    contributors = get_contributors(repo_info)
    print(f"Number of contributors ðŸ‘·: {len(contributors)}")

if __name__ == "__main__":
    log_repo_info()
```

<Note>
The `log_prints=True` argument provided to the `@flow` decorator automatically converts any `print` statements within the function to `INFO` level logs.
</Note>

## Run your flow

You can run your Prefect flow just as you would a Python script:

```bash
python my_gh_workflow.py
```

Prefect automatically tracks the state of the flow run and logs the output, which can be viewed directly in the terminal or in the UI.

## Create a work pool

Running a flow locally is a good start, but most use cases require a remote execution environment.
A [work pool](/3.0/deploy/infrastructure-concepts/work-pools/) is the most common interface for deploying flows to remote infrastructure.

<Tabs>
  <Tab title="Self-hosted">

Deploy your flow to a self-hosted Prefect server instance using a `Process` work pool.
All flow runs submitted to this work pool will run in a local subprocess (the mechanics are similar for other work pool types that run on remote infrastructure).

1. Create a `Process` work pool:

   ```bash
   prefect work-pool create --type process my-work-pool
   ```

1. Verify that the work pool exists:

   ```bash
   prefect work-pool ls
   ```

1. Start a worker to poll the work pool:

   ```bash
   prefect worker start --pool my-work-pool
   ```

  </Tab>
  <Tab title="Prefect Cloud">
Deploy your flow to Prefect Cloud using a managed work pool.

1. Create a [managed work pool](/3.0/deploy/infrastructure-concepts/work-pools):

   ```bash
   prefect work-pool create my-work-pool --type prefect:managed
   ```

1. View your new work pool on the **Work Pools** page of the UI.
  </Tab>
</Tabs>
  
<Tip>
You can also choose from other [work pool types](https://docs.prefect.io/concepts/work-pools/#worker-types).
</Tip>

## Deploy and schedule your flow

A [deployment](/3.0/deploy/infrastructure-examples/docker/) is used to determine when, where, and how a flow should run.
Deployments elevate flows to remotely configurable entities that have their own API.

1. Create a deployment in code:

   ```bash create_deployment.py
   from prefect import flow

   # Source for the code to deploy (here, a GitHub repo)
   SOURCE_REPO="https://github.com/prefecthq/demos.git"

   if __name__ == "__main__":
       flow.from_source(
           source=SOURCE_REPO,
           entrypoint="my_gh_workflow.py:repo_info", # Specific flow to run
       ).deploy(
           name="my-first-deployment",
           work_pool_name="my-work-pool", # Work pool target
           cron="0 1 * * *", # Cron schedule (1am every day)
       )
   ```

   <Tip>
   You can store your flow code in nearly any location as long as Prefect can access it.
   See [Where to store your flow code](/3.0/deploy/infrastructure-concepts/store-flow-code) for more details.
   </Tip>

1. Run the script to create the deployment:

   ```bash
   python create_deployment.py
   ```

   Check the logs to ensure your deployment was created:

   ```bash
   Successfully created/updated all deployments!
   ______________________________________________________
   |                    Deployments                     |  
   ______________________________________________________
   |    Name                       |  Status  | Details |
   ______________________________________________________
   | repo-info/my-first-deployment | applied  |         |
   ______________________________________________________
   ```

1. Schedule a run for the deployment:

   ```bash
   prefect deployment run 'repo-info/my-first-deployment'
   ```

   Soon you should see the flow run graph and logs on the **Flow Run** page in the UI.
   Logs are also streamed to the terminal.

   ![Flow run graph and logs](/3.0/img/ui/qs-flow-run.png)

## Next steps

You've seen how to move from a Python script to a scheduled, observable, remotely orchestrated workflow with Prefect.
Now consider reading: 

* [Write flows](/3.0/develop/write-flows)
* [Write tasks](/3.0/develop/write-tasks)
* [Cloud and server](/3.0/manage)
* [Manage infrastructure with work pools](/3.0/deploy/infrastructure-concepts/work-pools) to learn about running workflows on Kubernetes, Docker, and serverless infrastructure.

<Tip>
Need help? [Book a meeting](https://calendly.com/prefect-experts/prefect-product-advocates?utm_campaign=prefect_docs_cloud&utm_content=prefect_docs&utm_medium=docs&utm_source=docs) with a Prefect Product Advocate to get your questions answered.
</Tip>
