---
title: Extract data from websites
description: Use Prefect's error handling to deal with unstructured data
---

In [Build a data pipeline](/3.0/tutorials/pipelines), you learned how to build a resilient and performant data pipeline with Prefect.

Until now we've been working with structured data (GitHub stars), however, in the real world, you may need to work with unstructured data.
You can't make any assumptions about unstructured data and need to gracefully handle whatever data you get.
In this tutorial, instead of fetching stars, you'll fetch READMEs and summarize them.

Some problems that you'll learn to solve include:

- Error handling
    - Demonstrate the use of try/catch blocks to set the flow state to `Failed` if an error occurs
    - Use logging to highlight any errors that arose during data processing
    - Use transactions to rollback if the README processing pipeline fails partway through
- How to handle big data
    - Save data to disk instead of passing results between tasks directly
    - Use artifacts to note the data which failed to process, so that you can fix the data pipeline and reprocess the failures without having to restart the entire data pipeline from scratch.
    - Task runners for distributed compute? (probably out of scope for a tutorial like this one)

Some complexities related to READMEs specifically:

- How to deal with repositories that lack a README.
- How to find READMEs that don't follow normal naming conventions.
- How to process different content formats: plaintext, Markdown, reStructuredText, etc.
- How to handle READMEs that are too large to process in one go.
- How to incorporate linked assets like images, videos, or documentation in the summarization model (requires multi-modal capabilities).

## Error handling

TBD

## Dealing with big data

TBD

## Run the web scraper

NOTE: This is not the final script. It's just a placeholder.

```python my_web_scraper.py
from datetime import timedelta
import base64
import httpx
from typing import Optional

from prefect import flow, task
from prefect.cache_policies import INPUTS
from prefect.concurrency.sync import rate_limit

# You'll need to install these dependencies
from transformers import pipeline
import markdown


@flow(log_prints=True)
def analyze_readmes(github_repos: list[str]):
    """Flow: Fetch and analyze README contents from GitHub repos"""
    
    # Task 1: Make HTTP requests in parallel while respecting rate limits
    repo_contents = []
    for repo in github_repos:
        rate_limit("github-api")
        repo_contents.append({
            'repo': repo,
            'task': fetch_readme.submit(repo)
        })
    
    # Task 2: Process README contents once all parallel tasks complete
    for repo in repo_contents:
        repo_name = repo['repo']
        readme_content = repo['task'].result()
        
        if readme_content:
            summary = summarize_text(readme_content).result()
            print(f"\n=== {repo_name} README Summary ===")
            print(summary)
        else:
            print(f"\n=== {repo_name}: No README found ===")


@task(retries=2, cache_policy=INPUTS, cache_expiration=timedelta(days=1))
def fetch_readme(github_repo: str) -> Optional[str]:
    """Task 1: Fetch and decode the README content from a GitHub repo
    
    Returns:
        str or None: Decoded README text if found, None if no README exists
    """
    # Try common README filenames
    readme_files = ["README.md", "README.rst", "README.txt", "README"]
    
    for filename in readme_files:
        try:
            response = httpx.get(
                f"https://api.github.com/repos/{github_repo}/contents/{filename}",
                timeout=10.0
            )
            
            if response.status_code == 200:
                content = response.json()
                
                # GitHub API returns content as base64 encoded
                if content.get("encoding") == "base64":
                    decoded_content = base64.b64decode(content["content"]).decode('utf-8')
                    
                    # Convert markdown to plain text if it's a markdown file
                    if filename.endswith('.md'):
                        html = markdown.markdown(decoded_content)
                        # Basic HTML tag removal (you might want a more robust solution)
                        text = (html.replace('<p>', '\n\n')
                                  .replace('</p>', '')
                                  .replace('<h1>', '\n\n')
                                  .replace('</h1>', '\n')
                                  .replace('<h2>', '\n\n')
                                  .replace('</h2>', '\n')
                                  .replace('<h3>', '\n\n')
                                  .replace('</h3>', '\n')
                                  .replace('<code>', '')
                                  .replace('</code>', ''))
                        return text
                    return decoded_content
                    
        except (httpx.RequestError, KeyError, UnicodeDecodeError) as e:
            continue
            
    return None


@task(retries=1)
def summarize_text(text: str, max_length: int = 1000) -> str:
    """Task 2: Summarize README text using a pretrained model
    
    Args:
        text: The README text to summarize
        max_length: Maximum length of text to process (to avoid memory issues)
    
    Returns:
        str: A summary of the README content
    """
    # Truncate very long texts to avoid memory issues
    if len(text) > max_length:
        text = text[:max_length] + "..."
    
    try:
        # Initialize the summarization pipeline
        summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
        
        # Generate summary
        summary = summarizer(text, max_length=130, min_length=30, do_sample=False)
        return summary[0]['summary_text']
        
    except Exception as e:
        # Fallback to a basic summary if model fails
        sentences = text.split('.')[:3]  # First 3 sentences
        return '. '.join(sentences) + '.'


# Run the flow
if __name__ == "__main__":
    analyze_readmes([
        "PrefectHQ/prefect",
        "tensorflow/tensorflow",
        "huggingface/transformers"
    ])
```

## Next steps

In this tutorial, you built a web scraper capable of handling unstructured data gracefully.

Now that you've finished this tutorial series, continue your learning journey by going deep on the following topics:

* [Write flows](/3.0/develop/write-flows)
* [Write tasks](/3.0/develop/write-tasks)
* [Cloud and server](/3.0/manage)
* [Manage infrastructure with work pools](/3.0/deploy/infrastructure-concepts/work-pools) to learn about running workflows on Kubernetes, Docker, and serverless infrastructure.

<Tip>
Need help? [Book a meeting](https://calendly.com/prefect-experts/prefect-product-advocates?utm_campaign=prefect_docs_cloud&utm_content=prefect_docs&utm_medium=docs&utm_source=docs) with a Prefect Product Advocate to get your questions answered.
</Tip>
