---
title: Schedule a flow
description: Deploy flows and run them on a schedule with Prefect.
---

In the [Quickstart](/v3/get-started/quickstart), you learned how to convert a Python script to a Prefect flow.

In this tutorial, you'll learn how to get that flow off of your local machine and run it on a schedule with Prefect.

## Publish your code to a remote repository

First, you need to take the code from your local machine and publish it to a remote repository.
We've already published the code to GitHub that you need for this tutorial:

```
https://github.com/prefecthq/demos.git
```

## Create a work pool

Running a flow locally is a good start, but most use cases require a remote execution environment.
A [work pool](/v3/deploy/infrastructure-concepts/work-pools/) is the most common interface for deploying flows to remote infrastructure.

<Tabs>
  <Tab title="Self-hosted">

Deploy your flow to a self-hosted Prefect server instance using a `Process` work pool.
All flow runs submitted to this work pool will run in a local subprocess (the creation mechanics are similar for other work pool types that run on remote infrastructure).

1. Create a `Process` work pool:

   ```bash
   prefect work-pool create --type process my-work-pool
   ```

1. Verify that the work pool exists:

   ```bash
   prefect work-pool ls
   ```

1. Start a worker to poll the work pool:

   ```bash
   prefect worker start --pool my-work-pool
   ```

  </Tab>
  <Tab title="Prefect Cloud">
Deploy your flow to Prefect Cloud using a managed work pool.

1. Create a [managed work pool](/v3/deploy/infrastructure-concepts/work-pools):

   ```bash
   prefect work-pool create my-work-pool --type prefect:managed
   ```

1. View your new work pool on the **Work Pools** page of the UI.
  </Tab>
</Tabs>
  
<Tip>
You can also choose from other [work pool types](/v3/deploy/infrastructure-concepts/work-pools#work-pool-types).
</Tip>

## Deploy and schedule your flow

A [deployment](/v3/deploy/infrastructure-examples/docker/) is used to determine when, where, and how a flow should run.
Deployments elevate flows to remotely configurable entities that have their own API.
To set a flow to run on a schedule, you need to create a deployment.

1. Create a deployment in code:

   ```python create_deployment.py
   from prefect import flow

   # Source for the code to deploy (here, a GitHub repo)
   SOURCE_REPO="https://github.com/prefecthq/demos.git"

   if __name__ == "__main__":
       flow.from_source(
           source=SOURCE_REPO,
           entrypoint="my_workflow.py:show_stars", # Specific flow to run
       ).deploy(
           name="my-first-deployment",
           parameters={
               "github_repos": [
                   "PrefectHQ/prefect",
                   "pydantic/pydantic",
                   "huggingface/transformers"
               ]
           },
           work_pool_name="my-work-pool",
           cron="0 * * * *",  # Run every hour
       )
   ```

   <Tip>
   You can store your flow code in nearly any location as long as Prefect can access it.
   See [Where to store your flow code](/v3/deploy/infrastructure-concepts/store-flow-code) for more details.
   </Tip>

1. Run the script to create the deployment:

   ```bash
   python create_deployment.py
   ```

   Check the logs to ensure your deployment was created:

   ```bash
   Successfully created/updated all deployments!
   _______________________________________________________
   |                    Deployments                      |
   _______________________________________________________
   |    Name                        |  Status  | Details |
   _______________________________________________________
   | show-stars/my-first-deployment | applied  |         |
   _______________________________________________________
   ```

1. Schedule a run for the deployment:

   ```bash
   prefect deployment run 'show-stars/my-first-deployment'
   ```

   Soon you should see the flow run graph and logs on the **Flow Run** page in the UI.
   Logs are also streamed to the terminal.

## Next steps

In this tutorial, you successfully deployed your flow to remote infrastructure and scheduled it to run automatically.

Next, learn how to build a [resilient and performant data pipeline](/v3/tutorials/pipelines) with retries, concurrent tasks, concurrency limits, and caching.

<Tip>
Need help? [Book a meeting](https://calendly.com/prefect-experts/prefect-product-advocates?utm_campaign=prefect_docs_cloud&utm_content=prefect_docs&utm_medium=docs&utm_source=docs) with a Prefect Product Advocate to get your questions answered.
</Tip>
