---
title: Migrate from Dagster
sidebarTitle: Dagster
description: A guide for migrating from Dagster to Prefect.
---

This guide helps you migrate from Dagster to Prefect, mapping key concepts and showing equivalent patterns.

## Concept mapping

| Dagster | Prefect | Notes |
|---------|---------|-------|
| Assets | [Assets](/v3/concepts/assets) | Prefect Assets (Cloud) track data lineage with `@materialize`. Similar intent-based model. |
| Ops | [Tasks](/v3/develop/write-tasks) | Both are decorated functions. Prefect tasks are simpler—no `In`/`Out` types needed. |
| Jobs / Graphs | [Flows](/v3/develop/write-flows) | Prefect flows are just Python functions that call tasks. |
| Resources | [Blocks](/v3/develop/blocks) | Prefect Blocks provide dependency injection with UI configuration and secret management. |
| Partitions | Parameters + [`.map()`](/v3/develop/task-runners#mapping-over-iterables) | Prefect uses parameters for partition keys and `.map()` for parallel execution. No built-in partition state tracking. |
| Schedules | [Deployment schedules](/v3/deploy/serve-flows/deploy-a-flow#scheduling) | Scheduling is configured at deployment time, not in flow code. |
| Sensors | [Automations](/v3/automate/events/automations-triggers) | Event-driven triggers. Can also use polling flows for custom logic. |
| `Definitions` | Implicit discovery | No explicit registration needed—Prefect discovers flows at deployment. |

### What's different

- **Data passing**: Dagster uses IO Managers; Prefect tasks return data directly in-memory.
- **Partition state**: Dagster tracks materialization status per partition in the UI. Prefect doesn't have this—you handle idempotency yourself.
- **Testing**: Dagster often requires context mocking. Prefect tasks/flows are regular Python functions—test them directly.

## Converting assets

Dagster assets represent data artifacts. Prefect has two relevant patterns:

### Using Prefect Assets (Cloud)

For lineage tracking similar to Dagster, use [Prefect Assets](/v3/concepts/assets):

{/* pmd-metadata: notest */}
```python
from prefect import flow
from prefect.assets import materialize

@materialize("s3://data-lake/raw/weather.parquet")
def fetch_weather_data():
    """Materialize weather data to S3."""
    return {"temperature": 72, "humidity": 45}

@materialize(
    "s3://data-lake/processed/metrics.parquet",
    asset_deps=["s3://data-lake/raw/weather.parquet"]
)
def compute_metrics(raw_data: dict):
    """Process raw data into metrics."""
    return {"avg_temp": raw_data["temperature"]}

@flow
def weather_pipeline():
    raw = fetch_weather_data()
    metrics = compute_metrics(raw)
    return metrics
```

### Using tasks with caching

For simpler cases without full lineage tracking, use [cache policies](/v3/concepts/caching):

```python
from prefect import task, flow
from prefect.cache_policies import INPUTS

@task(cache_policy=INPUTS)
def fetch_data(date: str) -> dict:
    """Cached based on inputs - won't recompute for same date."""
    return {"date": date, "value": 42}

@task
def process_data(raw: dict) -> dict:
    return {"processed": True, **raw}

@flow
def data_pipeline(date: str = "2024-01-01"):
    raw = fetch_data(date)
    return process_data(raw)

if __name__ == "__main__":
    data_pipeline()
```

## Converting ops to tasks

Dagster ops map directly to Prefect tasks. Remove the type machinery:

```python
from prefect import task, flow

@task
def extract() -> dict:
    return {"source": "api", "value": 42}

@task
def transform(data: dict) -> dict:
    return {"transformed": True, **data}

@task
def load(data: dict) -> None:
    print(f"Loading: {data}")

@flow
def etl_pipeline():
    raw = extract()
    processed = transform(raw)
    load(processed)

if __name__ == "__main__":
    etl_pipeline()
```

## Handling partitioned data

Dagster partitions provide built-in state tracking per partition. Prefect uses a different approach:

```python
from prefect import task, flow
from datetime import date, timedelta

@task
def process_date(partition_date: str) -> dict:
    """Process data for a single date."""
    return {"date": partition_date, "rows": 100}

@flow
def daily_pipeline(partition_date: str):
    """Run for a single partition."""
    return process_date(partition_date)

@flow
def backfill_pipeline(start: str, end: str):
    """Process a date range in parallel."""
    start_date = date.fromisoformat(start)
    end_date = date.fromisoformat(end)

    dates = []
    current = start_date
    while current <= end_date:
        dates.append(current.isoformat())
        current += timedelta(days=1)

    # Map runs tasks concurrently
    results = process_date.map(dates)
    return results

if __name__ == "__main__":
    # Single partition
    daily_pipeline("2024-06-15")

    # Backfill
    backfill_pipeline("2024-01-01", "2024-01-07")
```

<Note>
Unlike Dagster, Prefect doesn't track partition materialization state. If you need idempotent backfills, use [cache policies](/v3/concepts/caching) or check your data store before processing.
</Note>

## Resources to Blocks

Dagster's `ConfigurableResource` maps to Prefect [Blocks](/v3/develop/blocks):

```python
from prefect import task, flow
from prefect.blocks.system import Secret

@task
def fetch_from_api(endpoint: str) -> dict:
    """Use a secret block for API key."""
    # Load secret from Prefect (create via UI or CLI first)
    # api_key = Secret.load("weather-api-key")
    return {"endpoint": endpoint, "data": "..."}

@flow
def api_pipeline():
    return fetch_from_api("/weather")

if __name__ == "__main__":
    api_pipeline()
```

For database connections, use [integration packages](/integrations/integrations) like `prefect-sqlalchemy` which provide pre-built blocks.

## Schedules and deployment

Dagster defines schedules alongside jobs. Prefect decouples scheduling from flow code:

```python
from prefect import flow

@flow
def my_pipeline():
    return "done"

if __name__ == "__main__":
    # Schedule is set at deployment time, not in the flow
    my_pipeline.deploy(
        name="my-pipeline-daily",
        work_pool_name="my-pool",
        cron="0 6 * * *",
        timezone="America/Chicago",
    )
```

Or configure via `prefect.yaml`:

```yaml
deployments:
  - name: my-pipeline-daily
    entrypoint: flows/pipeline.py:my_pipeline
    work_pool:
      name: my-pool
    schedules:
      - cron: "0 6 * * *"
        timezone: "America/Chicago"
```

## Sensors to automations

For event-driven workflows, use [Automations](/v3/automate/events/automations-triggers) in the Prefect UI or create a polling flow:

```python
from prefect import flow, task
from prefect.deployments import run_deployment

@task
def check_for_new_files() -> list[str]:
    """Check for new data files."""
    # Your logic here - check S3, database, etc.
    return []

@flow
def file_sensor():
    """Poll for new files and trigger processing."""
    new_files = check_for_new_files()

    for file in new_files:
        run_deployment(
            name="process-file/production",
            parameters={"file_path": file},
        )

if __name__ == "__main__":
    # Deploy as a frequent schedule
    file_sensor.deploy(
        name="file-sensor",
        work_pool_name="my-pool",
        cron="*/5 * * * *",  # Every 5 minutes
    )
```

## Next steps

- [Install Prefect](/v3/get-started/install)
- [Learn about Prefect concepts](/v3/concepts/)
- [Deploy your first flow](/v3/deploy/serve-flows/deploy-a-flow)
- [Join the Prefect community](https://prefect.io/slack)
