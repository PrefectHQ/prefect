---
title: Migrate from Dagster
sidebarTitle: Dagster
description: A guide for migrating from Dagster to Prefect.
---

This guide maps Dagster concepts to their Prefect equivalents.

## Concept mapping

| Dagster | Prefect | Notes |
|---------|---------|-------|
| Software-Defined Assets | [Assets](/v3/concepts/assets) (Cloud) | Both track data lineage. Prefect uses `@materialize`; Dagster uses `@asset`. |
| Ops | [Tasks](/v3/develop/write-tasks) | Decorated functions. Prefect doesn't require `In`/`Out` type specifications. |
| Jobs | [Flows](/v3/develop/write-flows) | Units of execution. Prefect flows are regular Python functions. |
| Resources | [Blocks](/v3/concepts/blocks) | Dependency injection for external systems. Both support environment-specific config. |
| I/O Managers | None | Prefect tasks return data in-memory. Handle your own persistence. |
| Partitions | Parameters + `.map()` | Prefect has no built-in partition state tracking. See [caching](/v3/concepts/caching) for idempotency. |
| Schedules | [Deployment schedules](/v3/deploy/serve-flows/deploy-a-flow) | Prefect decouples schedules from code—set at deployment time. |
| Sensors | [Automations](/v3/automate/events/automations-triggers) | Event-driven triggers. Can also use polling flows. |
| Definitions | Implicit | No explicit registration needed. |

## Assets

Dagster Software-Defined Assets track data lineage:

{/* pmd-metadata: notest */}
```python
import dagster as dg

@dg.asset
def raw_data():
    return {"value": 42}

@dg.asset
def processed_data(raw_data):
    return {"processed": True, **raw_data}
```

Prefect Assets (Cloud) provide similar lineage tracking with `@materialize`:

{/* pmd-metadata: notest */}
```python
from prefect import flow
from prefect.assets import materialize

@materialize("s3://bucket/raw-data.parquet")
def extract_raw_data():
    return {"value": 42}

@materialize("s3://bucket/processed-data.parquet")
def process_data(raw):
    return {"processed": True, **raw}

@flow
def pipeline():
    raw = extract_raw_data()
    process_data(raw)  # dependency inferred from task graph
```

Both models track what data is produced and consumed. Prefect Assets require URIs identifying where data lives; dependencies are inferred from the task graph or declared with `asset_deps`.

For cross-flow or external dependencies:

{/* pmd-metadata: notest */}
```python
@materialize(
    "s3://bucket/enriched.parquet",
    asset_deps=["postgres://db/users", "s3://external/vendor-feed.csv"]
)
def enrich_data():
    pass
```

<Note>
Prefect Assets are Cloud-only. For open-source Prefect, use [cache policies](/v3/concepts/caching) for idempotent tasks, but you won't get UI lineage visualization.
</Note>

## Ops → Tasks

Dagster ops are decorated functions with explicit typing:

{/* pmd-metadata: notest */}
```python
import dagster as dg

@dg.op
def extract() -> dict:
    return {"source": "api", "value": 42}

@dg.op
def transform(data: dict) -> dict:
    return {"transformed": True, **data}
```

Prefect tasks are simpler—just decorated Python functions:

```python
from prefect import task, flow

@task
def extract() -> dict:
    return {"source": "api", "value": 42}

@task
def transform(data: dict) -> dict:
    return {"transformed": True, **data}

@flow
def etl():
    raw = extract()
    return transform(raw)

if __name__ == "__main__":
    etl()
```

## Jobs → Flows

Dagster jobs group assets or ops for execution:

{/* pmd-metadata: notest */}
```python
import dagster as dg

my_job = dg.define_asset_job("my_job", selection=[raw_data, processed_data])
```

Prefect flows are just Python functions:

```python
from prefect import task, flow

@task
def step_a():
    return "a"

@task
def step_b(x):
    return f"b({x})"

@flow
def my_pipeline():
    a = step_a()
    return step_b(a)

if __name__ == "__main__":
    my_pipeline()
```

No explicit job definition needed—the flow function is the unit of execution.

## Resources → Blocks

Dagster resources provide dependency injection:

{/* pmd-metadata: notest */}
```python
from dagster import ConfigurableResource, asset

class DatabaseResource(ConfigurableResource):
    host: str
    port: int

    def query(self, sql: str):
        pass

@asset
def my_asset(database: DatabaseResource):
    return database.query("SELECT * FROM users")
```

Prefect blocks are similar—typed configuration with secret handling:

```python
from prefect import task, flow
from prefect.blocks.system import Secret

@task
def fetch_data():
    # Load block at runtime
    # api_key = Secret.load("my-api-key")
    return {"data": "fetched"}

@flow
def pipeline():
    return fetch_data()

if __name__ == "__main__":
    pipeline()
```

Create blocks via UI, CLI (`prefect block create`), or Python. Use [integration packages](/integrations/integrations) for pre-built blocks (databases, cloud storage, etc.).

## I/O Managers

Dagster I/O Managers abstract data persistence:

{/* pmd-metadata: notest */}
```python
# With I/O manager, Dagster handles read/write automatically
@dg.asset
def clean_data(raw_data: pd.DataFrame) -> pd.DataFrame:
    return raw_data.fillna(0)
```

Prefect has no equivalent. Tasks return data in-memory; you handle persistence explicitly:

```python
from prefect import task, flow

@task
def extract() -> dict:
    # You read data yourself
    return {"data": "from_source"}

@task
def transform(data: dict) -> dict:
    return {"transformed": True, **data}

@task
def load(data: dict):
    # You write data yourself
    print(f"Saving: {data}")

@flow
def etl():
    raw = extract()
    clean = transform(raw)
    load(clean)

if __name__ == "__main__":
    etl()
```

This is more explicit but gives you full control over how data moves.

## Partitions

Dagster partitions provide state tracking per partition:

{/* pmd-metadata: notest */}
```python
import dagster as dg

daily = dg.DailyPartitionsDefinition(start_date="2024-01-01")

@dg.asset(partitions_def=daily)
def daily_data(context):
    date = context.partition_key
    return {"date": date}
```

Prefect uses parameters and `.map()` for similar patterns, but without built-in partition state:

```python
from prefect import task, flow
from datetime import date, timedelta

@task
def process_date(partition_date: str) -> dict:
    return {"date": partition_date, "rows": 100}

@flow
def daily_pipeline(partition_date: str):
    return process_date(partition_date)

@flow
def backfill(start: str, end: str):
    start_dt = date.fromisoformat(start)
    end_dt = date.fromisoformat(end)

    dates = []
    current = start_dt
    while current <= end_dt:
        dates.append(current.isoformat())
        current += timedelta(days=1)

    return process_date.map(dates)

if __name__ == "__main__":
    daily_pipeline("2024-06-15")
    backfill("2024-01-01", "2024-01-07")
```

<Note>
Prefect doesn't track which partitions have been materialized. For idempotency, use [cache policies](/v3/concepts/caching) or check your data store before processing.
</Note>

## Schedules

Dagster defines schedules alongside jobs:

{/* pmd-metadata: notest */}
```python
import dagster as dg

@dg.schedule(job=my_job, cron_schedule="0 6 * * *")
def daily_schedule():
    return {}
```

Prefect schedules are set at deployment time, separate from flow code:

```python
from prefect import flow

@flow
def my_pipeline():
    return "done"

if __name__ == "__main__":
    my_pipeline.deploy(
        name="daily-run",
        work_pool_name="my-pool",
        cron="0 6 * * *",
        timezone="America/Chicago",
    )
```

Or via `prefect.yaml`:

```yaml
deployments:
  - name: daily-run
    entrypoint: pipeline.py:my_pipeline
    work_pool:
      name: my-pool
    schedules:
      - cron: "0 6 * * *"
        timezone: "America/Chicago"
```

## Sensors

Dagster sensors poll for events:

{/* pmd-metadata: notest */}
```python
import dagster as dg

@dg.sensor(job=my_job, minimum_interval_seconds=30)
def file_sensor():
    new_files = check_for_new_files()
    if new_files:
        for f in new_files:
            yield dg.RunRequest(run_key=f)
    else:
        yield dg.SkipReason("No new files")
```

Prefect offers [Automations](/v3/automate/events/automations-triggers) for event-driven triggers (configure in UI), or create a polling flow:

```python
from prefect import task, flow
from prefect.deployments import run_deployment

@task
def check_for_files() -> list[str]:
    # Your logic here
    return []

@flow
def file_sensor():
    files = check_for_files()
    for f in files:
        run_deployment(
            name="process-file/prod",
            parameters={"file": f},
        )

if __name__ == "__main__":
    file_sensor.deploy(
        name="file-sensor",
        work_pool_name="my-pool",
        cron="*/5 * * * *",
    )
```

## Definitions

Dagster requires explicit registration in a `Definitions` object:

{/* pmd-metadata: notest */}
```python
import dagster as dg

defs = dg.Definitions(
    assets=[asset_a, asset_b],
    jobs=[my_job],
    schedules=[daily_schedule],
    resources={"db": DatabaseResource(host="localhost", port=5432)},
)
```

Prefect doesn't require this—flows are discovered when deployed:

```bash
prefect deploy --all  # discovers flows from prefect.yaml
```

## Next steps

- [Install Prefect](/v3/get-started/install)
- [Write your first flow](/v3/develop/write-flows)
- [Deploy flows](/v3/deploy/serve-flows/deploy-a-flow)
