---
title: Migrate from Dagster
sidebarTitle: Dagster
description: A guide for migrating from Dagster to Prefect.
---

This guide helps Dagster users translate their mental model to Prefect. Each section explains what problem a concept solves, then shows how both frameworks approach it.

## Concept mapping

| Dagster | Prefect | Notes |
|---------|---------|-------|
| Software-Defined Assets | [Assets](/v3/concepts/assets) (Cloud) | Both track data lineage. Prefect uses `@materialize`; Dagster uses `@asset`. |
| Ops | [Tasks](/v3/develop/write-tasks) | Decorated functions. Prefect doesn't require `In`/`Out` type specifications. |
| Jobs | [Flows](/v3/develop/write-flows) | Execution units. Prefect flows are Python functions—no explicit job definition. |
| Resources | [Blocks](/v3/develop/blocks) | Dependency injection for external systems. Both support environment-specific config. |
| I/O Managers | [Result persistence](/v3/advanced/results) | Both decouple storage from logic. Prefect uses blocks + result storage config. |
| Partitions | Parameters + `.map()` | Prefect has no built-in partition state. See [caching](/v3/concepts/caching). |
| Schedules | [Deployment schedules](/v3/deploy/serve-flows/deploy-a-flow) | Prefect decouples schedules from code—set at deployment time. |
| Sensors | [Automations](/v3/automate/events/automations-triggers) | Event-driven triggers. Can also use polling flows. |
| Definitions | Implicit | No explicit registration needed—flows discovered at deployment. |

## Assets

Data engineers use assets to track **what data exists** and **how it was produced**. When pipelines grow complex, understanding data lineage becomes essential for debugging, auditing, and impact analysis.

Both Dagster and Prefect (Cloud) provide asset tracking with automatic lineage visualization. The key difference: Dagster infers storage from I/O managers, while Prefect requires explicit URIs.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

@dg.asset
def raw_orders():
    """Fetch raw order data."""
    return [{"id": 1, "amount": 100}, {"id": 2, "amount": 200}]

@dg.asset
def order_metrics(raw_orders):
    """Compute metrics from orders."""
    total = sum(o["amount"] for o in raw_orders)
    return {"total_revenue": total, "order_count": len(raw_orders)}
```

{/* pmd-metadata: notest */}
```python Prefect
from prefect import flow
from prefect.assets import materialize

@materialize("s3://data/raw-orders.json")
def raw_orders():
    """Fetch raw order data."""
    return [{"id": 1, "amount": 100}, {"id": 2, "amount": 200}]

@materialize("s3://data/order-metrics.json")
def order_metrics(raw_orders):
    """Compute metrics from orders."""
    total = sum(o["amount"] for o in raw_orders)
    return {"total_revenue": total, "order_count": len(raw_orders)}

@flow
def pipeline():
    orders = raw_orders()
    order_metrics(orders)
```
</CodeGroup>

Prefect infers dependencies from the task graph. For cross-flow or external dependencies, declare them explicitly:

{/* pmd-metadata: notest */}
```python
@materialize(
    "s3://data/enriched-orders.json",
    asset_deps=["postgres://db/customers", "s3://external/exchange-rates.csv"]
)
def enriched_orders():
    pass
```

<Note>
Prefect Assets require Prefect Cloud. For open-source Prefect, use [cache policies](/v3/concepts/caching) for idempotency, but you won't get UI lineage visualization.
</Note>

## Ops and tasks

Both frameworks use decorated functions as the unit of work. Data engineers use these to encapsulate discrete operations—fetching data, transforming it, loading it elsewhere.

The main difference: Dagster ops historically required explicit input/output type declarations (though this is now optional). Prefect tasks are plain Python functions.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

@dg.op
def fetch_orders() -> list[dict]:
    return [{"id": 1, "amount": 100}]

@dg.op
def compute_total(orders: list[dict]) -> int:
    return sum(o["amount"] for o in orders)
```

```python Prefect
from prefect import task, flow

@task
def fetch_orders() -> list[dict]:
    return [{"id": 1, "amount": 100}]

@task
def compute_total(orders: list[dict]) -> int:
    return sum(o["amount"] for o in orders)

@flow
def pipeline():
    orders = fetch_orders()
    return compute_total(orders)

if __name__ == "__main__":
    pipeline()
```
</CodeGroup>

## Jobs and flows

Data engineers group operations into executable units. In Dagster, you explicitly define jobs that select which assets or ops to run. In Prefect, the flow function *is* the job—no separate definition needed.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

# Assets defined elsewhere...

# Explicit job definition selecting assets
orders_job = dg.define_asset_job(
    name="orders_job",
    selection=[raw_orders, order_metrics],
)
```

```python Prefect
from prefect import task, flow

@task
def fetch_orders():
    return [{"id": 1, "amount": 100}]

@task
def compute_metrics(orders):
    return {"total": sum(o["amount"] for o in orders)}

# The flow IS the job - no separate definition
@flow
def orders_pipeline():
    orders = fetch_orders()
    return compute_metrics(orders)

if __name__ == "__main__":
    orders_pipeline()
```
</CodeGroup>

## Resources and blocks

Data engineers need a way to inject external dependencies—databases, APIs, cloud services—that can be swapped between environments (dev vs prod) without changing pipeline code.

Both frameworks solve this with typed configuration classes. Dagster calls them Resources; Prefect calls them Blocks.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
from dagster import ConfigurableResource, asset

class DatabaseResource(ConfigurableResource):
    host: str
    port: int

    def query(self, sql: str) -> list[dict]:
        # Connect and query...
        return []

@asset
def user_data(database: DatabaseResource):
    return database.query("SELECT * FROM users")
```

```python Prefect
from prefect import task, flow
from prefect.blocks.core import Block

class DatabaseBlock(Block):
    host: str
    port: int

    def query(self, sql: str) -> list[dict]:
        # Connect and query...
        return []

@task
def fetch_user_data() -> list[dict]:
    db = DatabaseBlock.load("prod-db")
    return db.query("SELECT * FROM users")

@flow
def pipeline():
    return fetch_user_data()

if __name__ == "__main__":
    pipeline()
```
</CodeGroup>

Create and save blocks via Python, CLI, or UI:

{/* pmd-metadata: notest */}
```python
# Save a block for later use
db = DatabaseBlock(host="localhost", port=5432)
db.save("dev-db", overwrite=True)
```

See [creating custom blocks](/v3/advanced/custom-blocks) for more details. Prefect also provides [pre-built blocks](/integrations/integrations) for common services (AWS, GCP, databases, etc.).

## I/O managers and result persistence

Data engineers want to separate **what** a pipeline computes from **where** it stores results. This lets you swap storage backends between environments without changing pipeline code.

Dagster solves this with I/O Managers. Prefect uses **result persistence** with storage blocks.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg
from dagster_aws.s3.io_manager import S3PickleIOManager

@dg.asset
def cleaned_orders(raw_orders):
    return [o for o in raw_orders if o["amount"] > 0]

defs = dg.Definitions(
    assets=[cleaned_orders],
    resources={
        "io_manager": S3PickleIOManager(
            s3_bucket="my-bucket",
            s3_prefix="data",
        )
    },
)
```

```python Prefect
from prefect import task, flow

@task(result_storage="s3-bucket/my-results")
def cleaned_orders(raw_orders: list[dict]) -> list[dict]:
    return [o for o in raw_orders if o["amount"] > 0]

@flow
def pipeline():
    raw = [{"id": 1, "amount": 100}, {"id": 2, "amount": -50}]
    return cleaned_orders(raw)

if __name__ == "__main__":
    pipeline()
```
</CodeGroup>

Set a default storage backend globally:

```bash
prefect config set PREFECT_DEFAULT_RESULT_STORAGE_BLOCK='s3-bucket/my-results'
```

See [result persistence](/v3/advanced/results) for more options.

## Partitions

Data engineers partition data by time or other dimensions to process incrementally. Dagster has first-class partition definitions with state tracking. In Prefect, you pass partition values as parameters.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

daily = dg.DailyPartitionsDefinition(start_date="2024-01-01")

@dg.asset(partitions_def=daily)
def daily_orders(context: dg.AssetExecutionContext):
    date = context.partition_key
    return fetch_orders_for_date(date)
```

```python Prefect
from prefect import task, flow

@task
def fetch_orders_for_date(date: str) -> dict:
    return {"date": date, "order_count": 42}

@flow
def daily_orders(date: str):
    return fetch_orders_for_date(date)

if __name__ == "__main__":
    daily_orders("2024-01-15")
```
</CodeGroup>

For backfills, use `.map()` to process multiple partitions in parallel:

{/* pmd-metadata: notest */}
```python
dates = ["2024-01-01", "2024-01-02", "2024-01-03"]
fetch_orders_for_date.map(dates)
```

<Note>
Prefect doesn't track partition state. Use [cache policies](/v3/concepts/caching) for idempotency.
</Note>

## Schedules

Both frameworks support cron-based scheduling. The key difference: Dagster defines schedules alongside code, while Prefect decouples them—schedules are set at deployment time.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

@dg.schedule(job=orders_job, cron_schedule="0 6 * * *")
def daily_orders_schedule():
    return {}
```

```python Prefect
from prefect import flow

@flow
def orders_pipeline():
    return "done"

if __name__ == "__main__":
    orders_pipeline.deploy(
        name="daily-orders",
        work_pool_name="my-pool",
        cron="0 6 * * *",
        timezone="America/Chicago",
    )
```
</CodeGroup>

Or configure schedules in `prefect.yaml`:

```yaml
deployments:
  - name: daily-orders
    entrypoint: pipeline.py:orders_pipeline
    work_pool:
      name: my-pool
    schedules:
      - cron: "0 6 * * *"
        timezone: "America/Chicago"
```

## Sensors

Data engineers use sensors for event-driven pipelines—triggering runs when new files arrive, database rows change, or external events occur.

Dagster sensors poll for events. Prefect offers [Automations](/v3/automate/events/automations-triggers) for event-driven triggers, or you can create a polling flow.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

@dg.sensor(job=process_file_job, minimum_interval_seconds=30)
def new_file_sensor(context: dg.SensorEvaluationContext):
    new_files = check_for_new_files()
    for f in new_files:
        yield dg.RunRequest(run_key=f, run_config={"file": f})
    if not new_files:
        yield dg.SkipReason("No new files")
```

```python Prefect
from prefect import task, flow
from prefect.deployments import run_deployment

@task
def check_for_new_files() -> list[str]:
    # Check S3, database, API, etc.
    return []

@flow
def file_sensor():
    new_files = check_for_new_files()
    for f in new_files:
        run_deployment(
            name="process-file/prod",
            parameters={"file": f},
        )

if __name__ == "__main__":
    file_sensor.deploy(
        name="file-sensor",
        work_pool_name="my-pool",
        cron="*/5 * * * *",
    )
```
</CodeGroup>

For push-based events, use [webhooks](/v3/concepts/webhooks) or [Automations](/v3/automate/events/automations-triggers) in the Prefect UI.

## Definitions

Dagster requires explicit registration of all assets, jobs, schedules, and resources in a `Definitions` object. Prefect discovers flows automatically when you deploy.

<CodeGroup>

{/* pmd-metadata: notest */}
```python Dagster
import dagster as dg

defs = dg.Definitions(
    assets=[raw_orders, order_metrics],
    jobs=[orders_job],
    schedules=[daily_orders_schedule],
    resources={
        "database": DatabaseResource(host="localhost", port=5432),
    },
)
```

```bash Prefect
# Flows discovered from prefect.yaml
prefect deploy --all
```
</CodeGroup>

## Next steps

- [Install Prefect](/v3/get-started/install)
- [Write your first flow](/v3/develop/write-flows)
- [Deploy flows](/v3/deploy/serve-flows/deploy-a-flow)
