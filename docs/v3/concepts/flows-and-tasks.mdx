---
title: Flows and tasks
description: Flows and tasks are the building blocks of Prefect workflows
---
## Supported function types

Almost any standard Python function can be turned into a Prefect flow or task by adding the `@flow` or `@task` decorator. 

In particular, Prefect supports:

- Synchronous functions
- Asynchronous functions
- Instance methods
- Class methods
- Static methods
- Generators

Prefect uses client-side task run orchestration by default, which significantly improves performance, especially for workflows with many tasks. Task creation and state updates happen locally, reducing API calls to the Prefect server during execution. This enables efficient handling of large-scale workflows and improves reliability when server connectivity is intermittent.
<Tip>
Tasks are always executed in the main thread by default, unless a specific [task runner](/v3/develop/task-runners) is used to execute them on different threads, processes, or infrastructure. This facilitates native Python debugging and profiling.
</Tip>
Task updates are logged in batch, leading to eventual consistency for task states in the UI and API queries. While this means there may be a slight delay in seeing the most up-to-date task states, it allows for substantial performance improvements and increased workflow scale.

<Warning>
**Generator functions are consumed when returned from flows or tasks**

The result of a completed flow or task must be serializable, but generators cannot be serialized. 
Therefore, if you return a generator from a flow or task, the generator will be fully consumed and its yielded values will be returned as a list. 
This can lead to unexpected behavior or blocking if the generator is infinite or very large.

Here is an example of proactive generator consumption:

```python
from prefect import task


def gen():
    yield from [1, 2, 3]
    print('Generator consumed!')


@task
def f():
    return gen()


f()  # prints 'Generator consumed!'
```

If you need to return a generator without consuming it, you can `yield` it instead of using `return`. 
Values yielded from generator tasks are not considered final results and do not face the same serialization constraints:

```python
from prefect import task


def gen():
    yield from [1, 2, 3]
    print('Generator consumed!')


@task
def f():
    yield gen()


generator = next(f())
list(generator) # prints 'Generator consumed!'
```
</Warning>

## Flows

Flows are defined as decorated Python functions.
They can take inputs, perform work, and return a result.

When a function becomes a flow, it gains the following capabilities:

- Metadata about [flow runs](#flow-runs), such as run time and final state, is automatically tracked.
- Each [state](/v3/develop/manage-states/) the flow enters is recorded. This
allows you to observe and [act upon each transition](/v3/develop/manage-states#execute-code-on-state-changes) in the flow's execution.
- Input arguments can be type validated as workflow [parameters](#specify-flow-parameters).
- [Retries](#retries) can be performed on failure, with configurable delay and retry limits.
- Timeouts can be enforced to prevent unintentional, long-running workflows.
- A flow can be [deployed](/v3/deploy/infrastructure-examples/docker/), which exposes an API for interacting with it remotely.

Flows are uniquely identified by name.

### Running a flow

A _flow run_ is a single execution of a flow.

Create a flow run by calling a `@flow` decorated function, just as you would a normal Python function.

You can also create a flow run by:

- Using external schedulers such as `cron` to invoke a flow function
- Triggering a [deployment](/v3/deploy/infrastructure-examples/docker/) of that flow in Prefect Cloud or self-hosted Prefect server
- Starting a flow run for the deployment through a schedule, the Prefect UI, or the Prefect API

However you run your flow, Prefect monitors the flow run, capturing its state for observability.
You can log a [variety of metadata](/v3/develop/logging) about flow runs for monitoring, troubleshooting, and auditing purposes.

### Specify flow parameters

As with any Python function, you can pass arguments to a flow, including both positional and keyword arguments.
These arguments defined on your flow function are called parameters.
They are stored by the Prefect orchestration engine on the flow run object.

Prefect automatically performs type conversion of inputs using any provided type hints.
Type hints provide a simple way to enforce typing on your flow parameters and can be customized with [Pydantic](https://pydantic-docs.helpmanual.io/).
Prefect supports any Pydantic model as a type hint for a flow parameter.

{/* MOVE TO DEPLOYMENT HOW-TO */}

Note that you can provide parameter values to a flow through the API using a [deployment](/v3/deploy/).
Flow run parameters sent to the API are coerced to the appropriate types when possible.

<Warning>
**Prefect API requires keyword arguments**

When creating flow runs from the Prefect API, you must specify parameter names when overriding defaults.
The values passed cannot be positional.
</Warning>

Parameters are validated before a flow is run.
If a flow run for a deployment receives invalid parameters, it moves from a `Pending` state to a `Failed` state without entering a `Running` state.

<Note>
Flow run parameters cannot exceed `512kb` in size.
</Note>

For a deep dive into flow parameters, see the [form-building tutorial](/v3/tutorials/form-building).

{/* END MOVE TO DEPLOYMENT HOW-TO */}

### Organize flows with tasks

Flows can call [tasks](/v3/develop/write-tasks), the most granular units of orchestrated work in Prefect workflows.

A single flow function can contain all of your workflow's code.
However, if you put all of your workflow logic in a single flow function and any line of code fails, the entire flow fails and must be retried from the beginning.
The more granular you make your workflows, the better they can recover from failures and the easier you can find and fix issues.

Prefect tasks are well suited for parallel or distributed execution using distributed computation frameworks such as Dask or Ray.

### Nesting flows

In addition to calling tasks from a flow, flows can also call other flows.
A nested flow run is created when a flow function is called by another flow.
When one flow calls another, the calling flow run is the "parent" run, the called flow run is the "child" run.

In the UI, each child flow run is linked to its parent and can be individually observed.

For most purposes, nested flow runs behave just like unnested flow runs.
There is a full representation of the nested flow run in the backend as if it had been called separately.
Nested flow runs differ from normal flow runs in that they resolve any passed task futures into data.
This allows data to be passed from the parent flow run to a nested flow run easily.

When a nested flow run starts, it creates a new [task runner](/v3/develop/task-runners/) for any tasks it contains.
When the nested flow run completes, the task runner shuts down.
Nested flow runs block execution of the parent flow run until completion.
However, asynchronous nested flows can run concurrently with [AnyIO task groups](https://anyio.readthedocs.io/en/stable/tasks.html) or [asyncio.gather](https://docs.python.org/3/library/asyncio-task.html#id6).

The relationship between nested runs is recorded through a special task run in the parent flow run that represents the child flow run.
The `state_details` field of the task run representing the child flow run includes a `child_flow_run_id`.
The `state_details` field of the nested flow run includes a `parent_task_run_id`.

You can define multiple flows within the same file.
Whether running locally or through a [deployment](/v3/deploy/infrastructure-examples/docker/), you must indicate which flow is the entrypoint for a flow run.

<Warning>
**Cancel nested flow runs**

A nested flow run cannot be cancelled without cancelling its parent flow run.
If you need to be able to cancel a nested flow run independent of its parent flow run, we recommend deploying it separately and starting it with
the [run_deployment](https://reference.prefect.io/prefect/deployments/flow_runs/#prefect.deployments.flow_runs.run_deployment) method.
</Warning>

Some scenarios where you might want to define a nested flow rather than call tasks individually include:

- Observability: Nested flows, like any other flow run, have first-class observability within the Prefect UI and Prefect Cloud. You'll
see nested flows' status in the **Runs** dashboard rather than having to dig down into the tasks within a specific flow run.
See [Final state determination](#final-state-determination) for examples of using task state within flows.
- Conditional flows: If you have a group of tasks that run only under certain conditions, you can group them within a nested flow and
conditionally run the nested flow rather than each task individually.
- Parameters: Flows have first-class support for parameterization, making it easy to run the same group of tasks in different use
cases by simply passing different parameters to the nested flow in which they run.
- Task runners: Nested flows enable you to specify the task runner used for tasks within the flow. For example, to optimize
parallel execution of certain tasks with Dask, group them in a nested flow that uses the Dask task runner. You can use a different
task runner for each nested flow.

### Flow runs

A _flow run_ is a single execution of a flow.

You can create a flow run by calling the flow function manually, or even by using an external scheduler such as `cron` to invoke a flow function.
Most users run flows by creating a [deployment](/v3/deploy/) on Prefect Cloud or Prefect server and then scheduling a flow run for the deployment through a schedule, the Prefect UI, or the Prefect API.

However you run a flow, the Prefect API monitors the flow run and records information for monitoring, troubleshooting, and auditing.

### Flow settings

All flows can be configured by passing arguments to the decorator. Flows accept the following optional settings:

| Argument                                           | Description                                                                                                                                                                                                          |
| -------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `description`                                      | An optional string description for the flow. If not provided, the description is pulled from the docstring for the decorated function.                                                                          |
| `name`                                             | An optional name for the flow. If not provided, the name is inferred from the function.                                                                                                                         |
| `retries`                                          | An optional number of times to retry on flow run failure.                                                                                                                                                            |
| <span class="no-wrap">`retry_delay_seconds`</span> | An optional number of seconds to wait before retrying the flow after failure. This is only applicable if `retries` is nonzero.                                                                                       |
| `flow_run_name`                                    | An optional name to distinguish runs of this flow; this name can be provided as a string template with the flow's parameters as variables; you can also provide this name as a function that returns a string.       |
| `task_runner`                                      | An optional [task runner](/v3/develop/task-runners/) to use for task execution within the flow when you `.submit()` tasks. If not provided and you `.submit()` tasks, the `ThreadPoolTaskRunner` is used.         |
| `timeout_seconds`                                  | An optional number of seconds indicating a maximum runtime for the flow. If the flow exceeds this runtime, it is marked as failed. Flow execution may continue until the next task is called.                   |
| `validate_parameters`                              | Boolean indicating whether parameters passed to flows are validated by Pydantic. Default is `True`.                                                                                                                  |
| `version`                                          | An optional version string for the flow. If not provided, we will attempt to create a version string as a hash of the file containing the wrapped function. If the file cannot be located, the version will be null. |

### Final state determination

A state is a record of the status of a particular task run or flow run.
See the [manage states](/v3/develop/manage-states) page for more information.

import FinalFlowState from '/snippets/final-flow-state.mdx'

<FinalFlowState />

#### Return a future

If a flow returns one or more futures, the final state is determined based on the underlying states.

```python
from prefect import flow, task


@task
def always_fails_task():
    raise ValueError("I fail successfully")


@task
def always_succeeds_task():
    print("I'm fail safe!")
    return "success"


@flow
def always_succeeds_flow():
    x = always_fails_task.submit().result(raise_on_failure=False)
    y = always_succeeds_task.submit(wait_for=[x])
    return y


if __name__ == "__main__":
    always_succeeds_flow()
```

This flow run finishes in a **Completed** final state because the flow returns the future of the task that succeeds.

#### Return multiple states or futures

If a flow returns a mix of futures and states, the final state is determined by resolving all futures to states,
then determining if any of the states are not `COMPLETED`.

```python
from prefect import task, flow


@task
def always_fails_task():
    raise ValueError("I am bad task")


@task
def always_succeeds_task():
    return "foo"


@flow
def always_succeeds_flow():
    return "bar"


@flow
def always_fails_flow():
    x = always_fails_task()
    y = always_succeeds_task()
    z = always_succeeds_flow()
    return x, y, z
```

Running `always_fails_flow` fails because one of the three returned futures fails.

If multiple states are returned, they must be contained in a `set`, `list`, or `tuple`.

#### Return a manual state

If a flow returns a manually created state, the final state is determined based upon the return value.

```python
from prefect import task, flow
from prefect.states import Completed, Failed


@task
def always_fails_task():
    raise ValueError("I fail successfully")


@task
def always_succeeds_task():
    print("I'm fail safe!")
    return "success"


@flow
def always_succeeds_flow():
    x = always_fails_task.submit()
    y = always_succeeds_task.submit()
    if y.result() == "success":
        return Completed(message="I am happy with this result")
    else:
        return Failed(message="How did this happen!?")


if __name__ == "__main__":
    always_succeeds_flow()
```

If a flow run returns any other object, then it is recorded as `COMPLETED`

#### Custom named states

You can also create custom named states to provide more granularity in your flow run states.

For example, we could create a `Skipped` state to indicate that a flow run was skipped.

```python
from prefect import flow
from prefect.states import Completed

@flow
def my_flow(work_to_do: bool):
    if not work_to_do:
        return Completed(message="No work to do 💤", name="Skipped")
    else:
        return Completed(message="Work was done 💪")


if __name__ == "__main__":
    my_flow(work_to_do=False)
```

## Tasks
You can turn any Python function into a task by adding an `@task` decorator to it.
Tasks can:

- Take inputs, perform work, and return outputs
- Cache their execution across invocations
- Encapsulate workflow logic into reusable units across flows
- Receive metadata about upstream task dependencies and their state before running
- Use automatic [logging](/v3/develop/logging/) to capture runtime details, tags, 
and final state
- Execute concurrently
- Be defined in the same file as the flow or imported from modules
- Be called from flows or other tasks

Flows and tasks share some common features:

- They can be defined using their respective decorator, which accepts configuration settings 
(see all [task settings](/v3/develop/write-tasks/#task-configuration) and 
[flow settings](/v3/develop/write-flows/#flow-settings))
- They can have a name, description, and tags for organization and bookkeeping
- They provide retries, timeouts, and other hooks to handle failure and completion events

### Example task

Here's an example of a simple flow with a single task:

```python repo_info.py
import httpx
from prefect import flow, task
from typing import Optional


@task
def get_url(url: str, params: Optional[dict[str, any]] = None):
    response = httpx.get(url, params=params)
    response.raise_for_status()
    return response.json()


@flow(retries=3, retry_delay_seconds=5, log_prints=True)
def get_repo_info(repo_name: str = "PrefectHQ/prefect"):
    url = f"https://api.github.com/repos/{repo_name}"
    repo_stats = get_url(url)
    print(f"{repo_name} repository statistics 🤓:")
    print(f"Stars 🌠 : {repo_stats['stargazers_count']}")
    print(f"Forks 🍴 : {repo_stats['forks_count']}")


if __name__ == "__main__":
    get_repo_info()
```

Running that flow in the terminal results in output like this:

```bash
09:55:55.412 | INFO    | prefect.engine - Created flow run 'great-ammonite' for flow 'get-repo-info'
09:55:55.499 | INFO    | Flow run 'great-ammonite' - Created task run 'get_url-0' for task 'get_url'
09:55:55.500 | INFO    | Flow run 'great-ammonite' - Executing 'get_url-0' immediately...
09:55:55.825 | INFO    | Task run 'get_url-0' - Finished in state Completed()
09:55:55.827 | INFO    | Flow run 'great-ammonite' - PrefectHQ/prefect repository statistics 🤓:
09:55:55.827 | INFO    | Flow run 'great-ammonite' - Stars 🌠 : 12157
09:55:55.827 | INFO    | Flow run 'great-ammonite' - Forks 🍴 : 1251
09:55:55.849 | INFO    | Flow run 'great-ammonite' - Finished in state Completed('All states completed.')
```

This task run is tracked in the UI as well.

Tasks are uniquely identified by a task key, which is a hash composed of the task name, the fully qualified name of the function, and any tags. 
If the task does not have a name specified, the name is derived from the decorated function object.

<Note>
**How big should a task be?**

Prefect encourages "small tasks." Each one should represent a single logical step of your workflow. 
This allows Prefect to better contain task failures and offer more granular observability and control of results.

There's nothing stopping you from putting all of your code in a single task. However, if any line of 
code fails, the entire task fails and must be retried from the beginning. 
Avoid this by splitting the code into multiple dependent tasks.
</Note>


### Concurrency

You can submit tasks for concurrent execution using the `.submit()` method.
This concurrency can greatly enhance the efficiency and performance of your workflows.

To motivate task concurrency, let's imagine expanding the [example script](/v3/develop/write-tasks/#example-task) to calculate the average open issues per user by making more requests:

```python repo_info.py
import httpx
from datetime import timedelta
from prefect import flow, task
from prefect.tasks import task_input_hash
from typing import Optional


@task(cache_key_fn=task_input_hash, cache_expiration=timedelta(hours=1))
def get_url(url: str, params: Optional[dict[str, any]] = None):
    response = httpx.get(url, params=params)
    response.raise_for_status()
    return response.json()


def get_open_issues(repo_name: str, open_issues_count: int, per_page: int = 100):
    issues = []
    pages = range(1, -(open_issues_count // -per_page) + 1)
    for page in pages:
        issues.append(
            get_url(
                f"https://api.github.com/repos/{repo_name}/issues",
                params={"page": page, "per_page": per_page, "state": "open"},
            )
        )
    return [i for p in issues for i in p]


@flow(retries=3, retry_delay_seconds=5, log_prints=True)
def get_repo_info(repo_name: str = "PrefectHQ/prefect"):
    repo_stats = get_url(f"https://api.github.com/repos/{repo_name}")
    issues = get_open_issues(repo_name, repo_stats["open_issues_count"])
    issues_per_user = len(issues) / len(set([i["user"]["id"] for i in issues]))
    print(f"{repo_name} repository statistics 🤓:")
    print(f"Stars 🌠 : {repo_stats['stargazers_count']}")
    print(f"Forks 🍴 : {repo_stats['forks_count']}")
    print(f"Average open issues per user 💌 : {issues_per_user:.2f}")


if __name__ == "__main__":
    get_repo_info()

```

Now you're fetching the data you need, but the requests happen sequentially.


Tasks expose a [`submit`](https://reference.prefect.io/prefect/tasks/#prefect.tasks.Task.submit) method that changes 
the execution from sequential to concurrent.
In this example, you also need to use the 
[`result`](https://reference.prefect.io/prefect/futures/#prefect.futures.PrefectFuture.result) 
method to unpack a list of return values:

```python 
def get_open_issues(repo_name: str, open_issues_count: int, per_page: int = 100):
    issues = []
    pages = range(1, -(open_issues_count // -per_page) + 1)
    for page in pages:
        issues.append(
            get_url.submit(
                f"https://api.github.com/repos/{repo_name}/issues",
                params={"page": page, "per_page": per_page, "state": "open"},
            )
        )
    return [i for p in issues for i in p.result()]
```

<Accordion title="Logs showing concurrent task execution">

```bash
12:45:28.241 | INFO    | prefect.engine - Created flow run 'intrepid-coua' for flow 'get-repo-info'
12:45:28.311 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-0' for task 'get_url'
12:45:28.312 | INFO    | Flow run 'intrepid-coua' - Executing 'get_url-0' immediately...
12:45:28.543 | INFO    | Task run 'get_url-0' - Finished in state Completed()
12:45:28.583 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-1' for task 'get_url'
12:45:28.584 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-1' for execution.
12:45:28.594 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-2' for task 'get_url'
12:45:28.594 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-2' for execution.
12:45:28.609 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-4' for task 'get_url'
12:45:28.610 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-4' for execution.
12:45:28.624 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-5' for task 'get_url'
12:45:28.625 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-5' for execution.
12:45:28.640 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-6' for task 'get_url'
12:45:28.641 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-6' for execution.
12:45:28.708 | INFO    | Flow run 'intrepid-coua' - Created task run 'get_url-3' for task 'get_url'
12:45:28.708 | INFO    | Flow run 'intrepid-coua' - Submitted task run 'get_url-3' for execution.
12:45:29.096 | INFO    | Task run 'get_url-6' - Finished in state Completed()
12:45:29.565 | INFO    | Task run 'get_url-2' - Finished in state Completed()
12:45:29.721 | INFO    | Task run 'get_url-5' - Finished in state Completed()
12:45:29.749 | INFO    | Task run 'get_url-4' - Finished in state Completed()
12:45:29.801 | INFO    | Task run 'get_url-3' - Finished in state Completed()
12:45:29.817 | INFO    | Task run 'get_url-1' - Finished in state Completed()
12:45:29.820 | INFO    | Flow run 'intrepid-coua' - PrefectHQ/prefect repository statistics 🤓:
12:45:29.820 | INFO    | Flow run 'intrepid-coua' - Stars 🌠 : 12159
12:45:29.821 | INFO    | Flow run 'intrepid-coua' - Forks 🍴 : 1251
Average open issues per user 💌 : 2.27
12:45:29.838 | INFO    | Flow run 'intrepid-coua' - Finished in state Completed('All states completed.')
```

</Accordion>

Read more about implementing task concurrency in the [Task Runners](/v3/develop/task-runners) guide.

### Task configuration

Tasks allow for customization through optional arguments that can be provided to the [task decorator](https://reference.prefect.io/prefect/tasks/#prefect.tasks.task).

| Argument              | Description                                                                                                                                                                                                             |
| --------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `name`                | An optional name for the task. If not provided, the name is inferred from the function name.                                                                                                                       |
| `description`         | An optional string description for the task. If not provided, the description is pulled from the docstring for the decorated function.                                                                             |
| `tags`                | An optional set of tags associated with runs of this task. These tags are combined with any tags defined by a `prefect.tags` context at task runtime.                                                             |
| `timeout_seconds`     | An optional number of seconds indicating a maximum runtime for the task. If the task exceeds this runtime, it will be marked as failed. |
| `cache_key_fn`        | An optional callable that, given the task run context and call parameters, generates a string key. If the key matches a previous completed state, that state result is restored instead of running the task again. |
| `cache_policy`        | An optional policy that determines what information is used to generate cache keys. Available policies include `INPUTS`, `TASK_SOURCE`, `RUN_ID`, `FLOW_PARAMETERS`, and `NO_CACHE`. Can be combined using the + operator. |
| `cache_expiration`    | An optional amount of time indicating how long cached states for this task are restorable; if not provided, cached states will never expire. |
| `retries`             | An optional number of times to retry on task run failure. |
| `retry_delay_seconds` | An optional number of seconds to wait before retrying the task after failure. This is only applicable if `retries` is nonzero.                                                                                          |
| `log_prints`|An optional boolean indicating whether to log print statements. |

See all possible options in the [Python SDK docs](https://reference.prefect.io/prefect/tasks/#prefect.tasks.task).

For example, provide optional `name` and `description` arguments to a task:

```python 
from prefect import task


@task(name="hello-task", description="This task says hello.")
def my_task():
    print("Hello, I'm a task")
```

Distinguish runs of this task by providing a `task_run_name`.  
Python's standard string formatting syntax applies:

```python
import datetime
from prefect import flow, task


@task(name="My Example Task", 
      description="An example task for a tutorial.",
      task_run_name="hello-{name}-on-{date:%A}")
def my_task(name, date):
    pass


@flow
def my_flow():
    # creates a run with a name like "hello-marvin-on-Thursday"
    my_task(name="marvin", date=datetime.datetime.now(datetime.timezone.utc))

if __name__ == "__main__":
    my_flow()
```

Additionally, this setting accepts a function that returns a string for the task run name:

```python
import datetime
from prefect import flow, task


def generate_task_name():
    date = datetime.datetime.now(datetime.timezone.utc)
    return f"{date:%A}-is-a-lovely-day"


@task(name="My Example Task",
      description="An example task for the docs.",
      task_run_name=generate_task_name)
def my_task(name):
    pass


@flow
def my_flow():
    # creates a run with a name like "Thursday-is-a-lovely-day"
    my_task(name="marvin")


if __name__ == "__main__":
    my_flow()  
```

### Tags

Tags are optional string labels that enable you to identify and group tasks other than by name or flow. 
Tags are useful to:

- Filter task runs by tag in the UI and through the [Prefect REST API](/v3/api-ref/rest-api/#filtering).
- Set [concurrency limits](#task-run-concurrency-limits) on task runs by tag.

You may specify tags as a keyword argument on the [task decorator](https://reference.prefect.io/prefect/tasks/#prefect.tasks.task).

```python 
from prefect import task


@task(name="hello-task", tags=["test"])
def my_task():
    print("Hello, I'm a task")
```

Alternatively, specify tags when the task is called rather than in its definition with a [`tags` context manager](https://reference.prefect.io/prefect/context/#prefect.context.tags), .

```python 
from prefect import flow, task
from prefect import tags


@task
def my_task():
    print("Hello, I'm a task")


@flow
def my_flow():
    with tags("test"):
        my_task()


if __name__ == "__main__":
    my_flow()
```

### Timeouts

Task timeouts prevent unintentional long-running tasks. When the duration of execution for a 
task exceeds the duration specified in the timeout, a timeout exception is raised and the task is  
marked as failed. In the UI, the task is visibly designated as `TimedOut`. From the perspective of the 
flow, the timed-out task is treated like any other failed task.

Specify timeout durations with the `timeout_seconds` keyword argument:

```python 
from prefect import task
import time


@task(timeout_seconds=1, log_prints=True)
def show_timeouts():
    print("I will execute")
    time.sleep(5)
    print("I will not execute")
```

### Retries

Prefect can automatically retry task runs on failure. 
A task run _fails_ if its Python function raises an exception.

To enable retries, pass `retries` and `retry_delay_seconds` arguments to your
task. 
If the task run fails, Prefect will retry it up to `retries` times, waiting
`retry_delay_seconds` seconds between each attempt. 
If the task fails on the final retry, Prefect marks the task as _failed_.

A new task run is not created when a task is retried. 
Instead, a new state is added to the state history of the original task run.

Retries are often useful in cases that depend upon external systems, such as making an API request. 

#### Custom retry behavior

The `retry_condition_fn` argument accepts a callable that returns a boolean. 
If the callable returns `True`, the task will be retried. 
If the callable returns `False`, the task will not be retried. 
The callable accepts three arguments: the task, the task run, and the state of the task run. 

Additionally, you can pass a callable that accepts the number of retries as an argument and returns a list. 
Prefect includes an [`exponential_backoff`](https://reference.prefect.io/prefect/tasks/#prefect.tasks.exponential_backoff) utility that will automatically generate a list of retry delays that correspond to an exponential backoff retry strategy. 

#### Add jitter to avoid thundering herds

You can add _jitter_ to retry delay times. 
Jitter is a random amount of time added to retry periods that helps prevent "thundering herd" scenarios, which is when many tasks retry at the same time, potentially overwhelming systems.

The `retry_jitter_factor` option can be used to add variance to the base delay. 
For example, a retry delay of 10 seconds with a `retry_jitter_factor` of 0.5 will allow a delay up to 15 seconds. 
Large values of `retry_jitter_factor` provide more protection against "thundering herds," while keeping the average retry delay time constant. 