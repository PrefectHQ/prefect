---
title: Run flows on heterogeneous infrastructure
description: Learn how to run different parts of your workflows on different infrastructure types
---

<Warning>
**Beta Feature**

This feature is currently in beta. While we encourage you to try it out and provide feedback, please be aware that the API may change in future releases, potentially including breaking changes.
</Warning>

# Run flows on heterogeneous infrastructure

Prefect allows you to run different parts of your workflow on different infrastructure types, enabling you to optimize resource usage and costs. This is particularly useful when parts of your workflow have different resource requirements, such as:

- Training machine learning models that require GPUs
- Processing large datasets that need significant memory
- Running lightweight tasks that can use minimal resources

## Benefits

Running workflows on heterogeneous infrastructure provides several advantages:

- **Resource optimization**: Dynamically allocate resources based on workflow requirements
- **Cost efficiency**: Use expensive infrastructure only when needed
- **Infrastructure flexibility**: Ensure workflows always run on the appropriate infrastructure type

## Prerequisites

Before binding your workflows to specific infrastructure, you'll need:

1. A work pool for each infrastructure type you want to use
2. Object storage to associate with your work pool(s)

## Setting up work pools and storage

### Creating a work pool

Create work pools for each infrastructure type using the Prefect CLI:

```bash
prefect work-pool create NAME --type WORK_POOL_TYPE
```

For detailed information on creating and configuring work pools, refer to the [work pools documentation](/v3/deploy/infrastructure-concepts/work-pools).

### Configuring storage

To enable Prefect to run workflows in remote infrastructure, work pools need an associated storage location to store serialized versions of submitted workflows.

Configure storage for your work pools using one of the supported storage types:

<CodeGroup>
```bash S3
prefect work-pool storage configure s3 WORK_POOL_NAME \
    --bucket BUCKET_NAME \
    --aws-credentials-block-name BLOCK_NAME
```

```bash Google Cloud Storage
prefect work-pool storage configure gcs WORK_POOL_NAME \
    --bucket BUCKET_NAME \
    --gcp-credentials-block-name BLOCK_NAME
```

```base Azure Blob Storage
prefect work-pool storage configure azure-blob-storage WORK_POOL_NAME \
    --container CONTAINER_NAME \
    --azure-blob-storage-credentials-block-name BLOCK_NAME
```
</CodeGroup>

To allow Prefect to upload and download serialized workflows, you can [create a block](/v3/develop/blocks) containing credentials with permission to access your configured storage location.

If a credentials block is not provided, Prefect will use the default credentials (e.g., a local profile or an IAM role) as determined by the corresponding cloud provider.

You can inspect your storage configuration using:

```bash
prefect work-pool storage inspect WORK_POOL_NAME
```

## Writing workflows for heterogeneous infrastructure

To run a flow on specific infrastructure, use the appropriate decorator for that infrastructure type.

Here's an example using `@kubernetes`:

```python
from prefect import flow
from prefect_kubernetes.experimental.decorators import kubernetes


# Bind `my_remote_flow` to run in a Kubernetes job
@kubernetes(work_pool="olympic")
@flow
def my_remote_flow(name: str):
    print(f"Hello {name}!")

@flow
def my_flow():
    my_remote_flow("Marvin")

# Run the flow
my_flow()
```

When you run this code on your machine, `my_flow` will execute locally, while `my_remote_flow` will run in a Kubernetes job.

<Note>
**Parameters must be serializable**

Parameters passed to infrastructure-bound flows are serialized with `cloudpickle` to allow them to be transported to the destination infrastructure. 

Most Python objects can be serialized with `cloudpickle`, but objects like database connections cannot be serialized. For parameters that cannot be serialized, you'll need to create the object inside your infrastructure-bound workflow.
</Note>

## Customizing infrastructure configuration

You can override the default configuration by providing additional kwargs to the infrastructure decorator:

```python
from prefect import flow
from prefect_kubernetes.experimental.decorators import kubernetes


@kubernetes(
    work_pool="my-kubernetes-pool",
    namespace="custom-namespace"
)
@flow
def custom_namespace_flow():
    pass
```

Any kwargs passed to the infrastructure decorator will override the corresponding default value in the [base job template](v3/deploy/infrastructure-concepts/work-pools#base-job-template) for the specified work pool.

## Retrieving results

If you want to return a value from an infrastructure-bound workflow, you'll need to [configure result storage](/v3/develop/results) for your workflow.

Here's an example with S3 result storage:

```python
from prefect import flow
from prefect_kubernetes.experimental.decorators import kubernetes


# Bind `my_remote_flow` to run in a Kubernetes job
@kubernetes(work_pool="olympic")
@flow(result_storage='s3-bucket/my-result-storage')
def my_remote_flow(name: str):
    return f"Hello {name}!"

result = my_remote_flow("Marvin")
print(result)  # prints "Hello Marvin!"
```

Return values must be serializable for storage and retrieval.
