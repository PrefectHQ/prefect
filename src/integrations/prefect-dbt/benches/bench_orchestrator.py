"""Benchmarks for PrefectDbtOrchestrator end-to-end performance.

Uses real dbt-DuckDB execution so the results reflect what users actually
experience: dbt invocation overhead, adapter initialisation, model execution,
and result collection.

Topology variants
-----------------
real_project    Existing 5-node test project (seeds + staging + marts).
                Represents a typical small project.
wide-N          N independent root models (all SELECT 1 AS id).
                One wave → one dbt invocation regardless of N.
                Tests how the orchestrator scales the single-invocation path.
linear-N        Linear chain of N models (model_0 → model_1 → … → model_N-1).
                N waves → N sequential dbt invocations.
                Tests per-wave dispatch overhead.

Pure-Python benchmarks (manifest parsing, wave computation) are also included
so regressions in the orchestration logic are visible even at larger scales
that would be too slow to run with real dbt invocations.
"""

from __future__ import annotations

import json
import shutil
from pathlib import Path
from typing import TYPE_CHECKING

import pytest
import yaml
from dbt.cli.main import dbtRunner
from prefect_dbt.core._orchestrator import (
    ExecutionMode,
    PrefectDbtOrchestrator,
    TestStrategy,
)
from prefect_dbt.core.settings import PrefectDbtSettings

if TYPE_CHECKING:
    from pytest_benchmark.fixture import BenchmarkFixture

pytest.importorskip("dbt.adapters.duckdb", reason="dbt-duckdb required for benchmarks")

DBT_TEST_PROJECT = Path(__file__).resolve().parent.parent / "tests" / "dbt_test_project"


# ---------------------------------------------------------------------------
# Helpers
# ---------------------------------------------------------------------------


def _dbt_parse(project_dir: Path) -> Path:
    runner = dbtRunner()
    result = runner.invoke(
        [
            "parse",
            "--project-dir",
            str(project_dir),
            "--profiles-dir",
            str(project_dir),
        ]
    )
    if not result.success:
        raise RuntimeError(f"dbt parse failed: {result.exception}")
    manifest_path = project_dir / "target" / "manifest.json"
    assert manifest_path.exists(), "manifest.json not generated by dbt parse"
    return manifest_path


def _duckdb_profiles(project_dir: Path, profile_name: str, threads: int = 4) -> dict:
    return {
        profile_name: {
            "target": "dev",
            "outputs": {
                "dev": {
                    "type": "duckdb",
                    "path": str(project_dir / "warehouse.duckdb"),
                    "schema": "main",
                    "threads": threads,
                }
            },
        }
    }


def _setup_wide_project(project_dir: Path, n: int) -> Path:
    """N independent root models — single wave regardless of N."""
    models_dir = project_dir / "models"
    models_dir.mkdir(parents=True)
    for i in range(n):
        (models_dir / f"model_{i}.sql").write_text("SELECT 1 AS id")
    project_name = f"bench_wide_{n}"
    (project_dir / "dbt_project.yml").write_text(
        yaml.dump(
            {
                "name": project_name,
                "version": "1.0.0",
                "config-version": 2,
                "profile": "bench",
                "model-paths": ["models"],
                "models": {project_name: {"+materialized": "table"}},
            }
        )
    )
    (project_dir / "profiles.yml").write_text(
        yaml.dump(_duckdb_profiles(project_dir, "bench", threads=min(n, 4)))
    )
    return _dbt_parse(project_dir)


def _setup_linear_project(project_dir: Path, n: int) -> Path:
    """Linear chain: model_0 → model_1 → … → model_{n-1}. One wave per node."""
    models_dir = project_dir / "models"
    models_dir.mkdir(parents=True)
    (models_dir / "model_0.sql").write_text("SELECT 1 AS id")
    for i in range(1, n):
        (models_dir / f"model_{i}.sql").write_text(
            f"SELECT id FROM {{{{ ref('model_{i - 1}') }}}}"
        )
    project_name = f"bench_linear_{n}"
    (project_dir / "dbt_project.yml").write_text(
        yaml.dump(
            {
                "name": project_name,
                "version": "1.0.0",
                "config-version": 2,
                "profile": "bench",
                "model-paths": ["models"],
                "models": {project_name: {"+materialized": "table"}},
            }
        )
    )
    (project_dir / "profiles.yml").write_text(
        yaml.dump(_duckdb_profiles(project_dir, "bench", threads=1))
    )
    return _dbt_parse(project_dir)


def _make_orchestrator(
    project_dir: Path, manifest_path: Path, **kwargs
) -> PrefectDbtOrchestrator:
    kwargs.setdefault("execution_mode", ExecutionMode.PER_WAVE)
    kwargs.setdefault("test_strategy", TestStrategy.SKIP)
    kwargs.setdefault("create_summary_artifact", False)
    kwargs.setdefault("write_run_results", False)
    settings = PrefectDbtSettings(
        project_dir=project_dir,
        profiles_dir=project_dir,
    )
    return PrefectDbtOrchestrator(
        settings=settings,
        manifest_path=manifest_path,
        **kwargs,
    )


# ---------------------------------------------------------------------------
# Session-scoped project fixtures (setup runs once per benchmark session)
# ---------------------------------------------------------------------------


@pytest.fixture(scope="session")
def real_project(tmp_path_factory: pytest.TempPathFactory) -> dict:
    """Existing 5-node test project (seeds + staging + marts) with DuckDB."""
    project_dir = tmp_path_factory.mktemp("real_project")
    for item in DBT_TEST_PROJECT.iterdir():
        dest = project_dir / item.name
        if item.is_dir():
            shutil.copytree(item, dest)
        else:
            shutil.copy2(item, dest)
    (project_dir / "profiles.yml").write_text(
        yaml.dump(
            {
                "test": {
                    "target": "dev",
                    "outputs": {
                        "dev": {
                            "type": "duckdb",
                            "path": str(project_dir / "warehouse.duckdb"),
                            "schema": "main",
                            "threads": 4,
                        }
                    },
                }
            }
        )
    )
    manifest_path = _dbt_parse(project_dir)
    return {"project_dir": project_dir, "manifest_path": manifest_path}


@pytest.fixture(
    scope="session",
    params=[10, 50],
    ids=["10", "50"],
)
def wide_project(
    request: pytest.FixtureRequest, tmp_path_factory: pytest.TempPathFactory
) -> dict:
    n = request.param
    project_dir = tmp_path_factory.mktemp(f"wide_{n}")
    manifest_path = _setup_wide_project(project_dir, n)
    return {"project_dir": project_dir, "manifest_path": manifest_path, "n": n}


@pytest.fixture(
    scope="session",
    params=[5, 10],
    ids=["5", "10"],
)
def linear_project(
    request: pytest.FixtureRequest, tmp_path_factory: pytest.TempPathFactory
) -> dict:
    n = request.param
    project_dir = tmp_path_factory.mktemp(f"linear_{n}")
    manifest_path = _setup_linear_project(project_dir, n)
    return {"project_dir": project_dir, "manifest_path": manifest_path, "n": n}


# ---------------------------------------------------------------------------
# End-to-end benchmarks — real dbt-DuckDB execution
# ---------------------------------------------------------------------------


def bench_run_build_real_project(
    benchmark: "BenchmarkFixture", real_project: dict
) -> None:
    """Full run_build() on the 5-node test project (seeds + staging + marts)."""
    orch = _make_orchestrator(
        real_project["project_dir"], real_project["manifest_path"]
    )
    benchmark.pedantic(orch.run_build, rounds=5, warmup_rounds=1)


def bench_run_build_wide(benchmark: "BenchmarkFixture", wide_project: dict) -> None:
    """run_build() with N independent root models — 1 wave, 1 dbt invocation."""
    orch = _make_orchestrator(
        wide_project["project_dir"], wide_project["manifest_path"]
    )
    benchmark.pedantic(orch.run_build, rounds=5, warmup_rounds=1)


def bench_run_build_linear(benchmark: "BenchmarkFixture", linear_project: dict) -> None:
    """run_build() for a linear chain — N waves, N sequential dbt invocations."""
    orch = _make_orchestrator(
        linear_project["project_dir"], linear_project["manifest_path"]
    )
    benchmark.pedantic(orch.run_build, rounds=5, warmup_rounds=1)


# ---------------------------------------------------------------------------
# Pure-Python benchmarks — no dbt execution
#
# These scale to graph sizes that would be impractical with real dbt and make
# regressions in orchestration logic visible independently of dbt performance.
# ---------------------------------------------------------------------------


def _wide_manifest_data(n: int) -> dict:
    return {
        "nodes": {
            f"model.bench.model_{i}": {
                "name": f"model_{i}",
                "resource_type": "model",
                "depends_on": {"nodes": []},
                "config": {"materialized": "table"},
            }
            for i in range(n)
        },
        "sources": {},
    }


def _linear_manifest_data(n: int) -> dict:
    nodes = {}
    for i in range(n):
        nodes[f"model.bench.model_{i}"] = {
            "name": f"model_{i}",
            "resource_type": "model",
            "depends_on": {"nodes": [f"model.bench.model_{i - 1}"] if i else []},
            "config": {"materialized": "table"},
        }
    return {"nodes": nodes, "sources": {}}


@pytest.mark.parametrize("n_nodes", [100, 500])
def bench_manifest_parse(
    benchmark: "BenchmarkFixture", n_nodes: int, tmp_path: Path
) -> None:
    """Time to parse a manifest of N nodes from disk."""
    from prefect_dbt.core._manifest import ManifestParser

    path = tmp_path / "manifest.json"
    path.write_text(json.dumps(_wide_manifest_data(n_nodes)))
    benchmark(ManifestParser, path)


@pytest.mark.parametrize("n_nodes", [100, 500])
def bench_wave_computation_linear(
    benchmark: "BenchmarkFixture", n_nodes: int, tmp_path: Path
) -> None:
    """Time to compute execution waves for a linear-chain DAG of N nodes."""
    from prefect_dbt.core._manifest import ManifestParser

    path = tmp_path / "manifest.json"
    path.write_text(json.dumps(_linear_manifest_data(n_nodes)))
    parser = ManifestParser(path)
    nodes = parser.filter_nodes()
    benchmark(parser.compute_execution_waves, nodes)
