name: Benchmarks

env:
  PY_COLORS: 1

on:
  pull_request:
    paths:
      - .github/workflows/benchmarks.yaml
      - .github/workflows/python-tests.yaml
      - benches/cli-bench.toml
      - "src/prefect/**/*.py"
      - pyproject.toml
      - uv.lock
      - Dockerfile
  push:
    branches:
      - main

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.event_name == 'pull_request' }}

jobs:
  run-benchmarks:
    name: Benchmark
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v6
        with:
          persist-credentials: false
          fetch-depth: 0

      - name: Set up uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: "3.12"
          enable-cache: true
          cache-dependency-glob: "pyproject.toml"

      - name: Install the project
        run:  uv sync --group benchmark --compile-bytecode --locked

      - name: Prepare benchmark comparisons
        # Note: We use a "cache" instead of artifacts because artifacts are not available
        #       across workflow runs.
        id: bench-cache
        uses: actions/cache@v5
        with:
          path: ./.benchmarks
          # Pushes benchmark results for this branch and sha, this will always be a cache miss
          # and `restore-keys` will be used to get the benchmarks for comparison
          key: ${{ runner.os }}-${{ github.head_ref || 'main' }}-${{ github.sha }}
          # Pulls benchmark results for the base branch
          restore-keys: |
            ${{ runner.os }}-${{ github.base_ref }}-
            ${{ runner.os }}-main-

      - name: Start server
        run: |
          PREFECT_HOME=$(pwd) uv run prefect server start&
          PREFECT_API_URL="http://127.0.0.1:4200/api" uv run ./scripts/wait-for-server.py

          # TODO: Replace `wait-for-server` with dedicated command
          #       https://github.com/PrefectHQ/prefect/issues/6990

      - name: Run benchmarks
        env:
          HEAD_REF: ${{ github.head_ref }}
          GITHUB_SHA: ${{ github.sha }}
        # Includes comparison to previous benchmarks if available
        # Import benchmark is ignored because because we run those
        # benchmarks via CodSpeed
        run: |
          if [[ -z "$HEAD_REF" ]]; then
            # HEAD_REF is unset or empty, use 'main' with the SHA
            uniquename="main-$GITHUB_SHA"
          else
            # HEAD_REF is set, use the branch name directly
            uniquename="$HEAD_REF"
          fi

          # Allow alphanumeric, underscores, and dashes, and replace other
          # characters with an underscore
          sanitized_uniquename="${uniquename//[^a-zA-Z0-9_\-]/_}"

          PREFECT_API_URL="http://127.0.0.1:4200/api" \
          uv run python -m benches \
          --ignore=benches/bench_import.py \
          --timeout=180 \
          --benchmark-save="${sanitized_uniquename}" \
          ${{ steps.bench-cache.outputs.cache-hit && '--benchmark-compare' || '' }}

  cli-benchmarks:
    name: CLI startup benchmarks
    if: ${{ github.event_name == 'pull_request' }}
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - uses: actions/checkout@v6
        with:
          fetch-depth: 0

      - name: Set up uv
        uses: astral-sh/setup-uv@v7
        with:
          python-version: "3.12"
          enable-cache: true
          cache-dependency-glob: "pyproject.toml"

      - name: Install hyperfine
        run: |
          sudo apt-get update
          sudo apt-get install -y hyperfine

      - name: Extract cli-bench config from head
        run: |
          git show ${{ github.event.pull_request.head.sha }}:benches/cli-bench.toml > "$RUNNER_TEMP/cli-bench.toml"
          echo "Using benches/cli-bench.toml from ${{ github.event.pull_request.head.sha }}"
          cat "$RUNNER_TEMP/cli-bench.toml"

      - name: Prepare worktrees
        run: |
          git fetch origin ${{ github.event.pull_request.base.sha }} ${{ github.event.pull_request.head.sha }}
          git worktree add "$RUNNER_TEMP/base" ${{ github.event.pull_request.base.sha }}
          git worktree add "$RUNNER_TEMP/head" ${{ github.event.pull_request.head.sha }}

      - name: Install dependencies (head)
        run: uv sync --group cli-bench --locked

      - name: Run baseline benchmarks (base)
        run: |
          uv run --group cli-bench cli-bench \
            --config "$RUNNER_TEMP/cli-bench.toml" \
            --project-root "$RUNNER_TEMP/base" \
            run \
            --runs 10 \
            --category startup \
            --output baseline.json

      - name: Run comparison benchmarks (head)
        run: |
          uv run --group cli-bench cli-bench \
            --config "$RUNNER_TEMP/cli-bench.toml" \
            --project-root "$RUNNER_TEMP/head" \
            run \
            --runs 10 \
            --category startup \
            --compare baseline.json \
            --output comparison.json

      - name: Check for regressions
        run: |
          python - <<'PYEOF'
          import json, math, sys
          from pathlib import Path

          THRESHOLD_PCT = 15  # fail if any command regresses more than this
          ALPHA = 0.05        # significance level for Welch's t-test

          baseline = json.loads(Path("baseline.json").read_text())
          comparison = json.loads(Path("comparison.json").read_text())

          base_by_cmd = {r["command"]: r for r in baseline["results"]}
          comp_by_cmd = {r["command"]: r for r in comparison["results"]}

          # all commands from both baseline and comparison
          all_cmds = list(dict.fromkeys(list(base_by_cmd) + list(comp_by_cmd)))

          header = "| Command | Base (ms) | Head (ms) | Delta | Sig? |"
          sep = "|---|---:|---:|---:|:---:|"
          rows = []
          regressions = []

          for cmd in all_cmds:
              base = base_by_cmd.get(cmd, {})
              comp = comp_by_cmd.get(cmd, {})
              b_warm = base.get("warm_cache") or {}
              c_warm = comp.get("warm_cache") or {}
              b_mean = b_warm.get("mean_ms")
              c_mean = c_warm.get("mean_ms")
              b_std = b_warm.get("stddev_ms")
              c_std = c_warm.get("stddev_ms")
              b_n = b_warm.get("runs")
              c_n = c_warm.get("runs")

              if c_mean is not None and b_mean is None:
                  rows.append(f"| {cmd} | (new) | {c_mean:.0f} | - | - |")
                  continue
              if b_mean is None or c_mean is None:
                  rows.append(f"| {cmd} | - | - | - | - |")
                  continue

              delta_pct = (c_mean - b_mean) / b_mean * 100

              # Welch's t-test (two-sample, unequal variance)
              sig = "?"
              if b_std and c_std and b_n and c_n and b_n > 1 and c_n > 1:
                  se = math.sqrt(b_std**2 / b_n + c_std**2 / c_n)
                  if se > 0:
                      t = (c_mean - b_mean) / se
                      # Welch-Satterthwaite degrees of freedom
                      num = (b_std**2 / b_n + c_std**2 / c_n) ** 2
                      den = (b_std**2 / b_n) ** 2 / (b_n - 1) + (c_std**2 / c_n) ** 2 / (c_n - 1)
                      df = num / den if den > 0 else 1
                      # approximate two-tailed p-value using normal (good enough for df>5)
                      z = abs(t)
                      p = math.erfc(z / math.sqrt(2))
                      sig = "yes" if p < ALPHA else "no"

                      if delta_pct > THRESHOLD_PCT and sig == "yes":
                          regressions.append((cmd, delta_pct, p))

              flag = " :red_circle:" if any(r[0] == cmd for r in regressions) else ""
              rows.append(f"| {cmd} | {b_mean:.0f} | {c_mean:.0f} | {delta_pct:+.1f}% | {sig}{flag} |")

          summary = []
          summary.append("## CLI Benchmark Results")
          summary.append("")
          summary.append(f"Threshold: {THRESHOLD_PCT}% regression (p < {ALPHA})")
          summary.append("")
          summary.append(header)
          summary.append(sep)
          summary.extend(rows)

          if regressions:
              summary.append("")
              summary.append(f"**{len(regressions)} significant regression(s) detected:**")
              for cmd, delta, p in regressions:
                  summary.append(f"- `{cmd}`: {delta:+.1f}% (p={p:.4f})")

          text = "\n".join(summary)

          import os
          summary_path = os.environ.get("GITHUB_STEP_SUMMARY", "")
          if summary_path:
              with open(summary_path, "a") as f:
                  f.write(text + "\n")

          print(text)

          if regressions:
              print(f"\n::error::CLI startup regression detected: {len(regressions)} command(s) exceeded {THRESHOLD_PCT}% threshold")
              sys.exit(1)
          PYEOF

      - name: Upload benchmark results
        if: always()
        uses: actions/upload-artifact@v6
        with:
          name: cli-benchmark-results
          path: |
            baseline.json
            comparison.json
